{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Swin Transformer in PyTorch",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'butterfly-images40-species:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F456014%2F3020675%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240924%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240924T122346Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D47911eacd71153317ecbf32ebb3e410f094dcf66bfbff7757065f76c64a7f359c2c455dbe22850a935646120cf74656fc3ee636492028efd99151505c6f7eb498cbca8eeb6a01194f6d13201fd25c2dcd0e4f095d8119d02b91133fc82884c2abf195cf8436379e72635c67d488c367180ae5e1161f0e88b0dd10cf8427e62421ebaa1d2822c6c441c890e8cad0fdec658ae43996381b352ff97d33098da102f966b78893fcf22f130d70606dad241af3e8cab9608ea30b28327a0ee2ff65e97e1aa2598fe137ebb5e2ae05915c5d14191f2d70accf33d8048de243ac0be01af151696d83ca9b78d302b8b95e04c5f38e9a42ea4da940990057be0c0e38532a3'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "U_IuFOyQ_OBU",
        "outputId": "a304505b-8913-4b83-a4f5-a9e6ac8b3c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading butterfly-images40-species, 382420015 bytes compressed\n",
            "[==================================================] 382420015 bytes downloaded\n",
            "Downloaded and uncompressed: butterfly-images40-species\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin Transformers\n",
        "\n",
        "This notebook trains a  Vision Transformer on the Butterfly dataset."
      ],
      "metadata": {
        "id": "8oBZIo3Z_OBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "# We use a butterfly dataset of 50 species to demonstrate the classification method\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-01-21T20:36:15.853625Z",
          "iopub.execute_input": "2022-01-21T20:36:15.853882Z",
          "iopub.status.idle": "2022-01-21T20:36:15.861107Z",
          "shell.execute_reply.started": "2022-01-21T20:36:15.853849Z",
          "shell.execute_reply": "2022-01-21T20:36:15.860388Z"
        },
        "trusted": true,
        "id": "WtbWj5uq_OBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T # for simplifying the transforms\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "from torchvision import models"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:36:15.864074Z",
          "iopub.execute_input": "2022-01-21T20:36:15.864284Z",
          "iopub.status.idle": "2022-01-21T20:36:15.870054Z",
          "shell.execute_reply.started": "2022-01-21T20:36:15.864255Z",
          "shell.execute_reply": "2022-01-21T20:36:15.869257Z"
        },
        "trusted": true,
        "id": "rmZIMBT5_OBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now, we import timm, torchvision image models\n",
        "!pip install timm # kaggle doesnt have it installed by default\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:36:15.871246Z",
          "iopub.execute_input": "2022-01-21T20:36:15.871611Z",
          "iopub.status.idle": "2022-01-21T20:36:23.068057Z",
          "shell.execute_reply.started": "2022-01-21T20:36:15.871556Z",
          "shell.execute_reply": "2022-01-21T20:36:23.067225Z"
        },
        "trusted": true,
        "id": "nUuBWAmj_OBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:36:23.071423Z",
          "iopub.execute_input": "2022-01-21T20:36:23.071643Z",
          "iopub.status.idle": "2022-01-21T20:36:23.075843Z",
          "shell.execute_reply.started": "2022-01-21T20:36:23.071617Z",
          "shell.execute_reply": "2022-01-21T20:36:23.075074Z"
        },
        "trusted": true,
        "id": "Qkn5g_TT_OBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:36:23.077021Z",
          "iopub.execute_input": "2022-01-21T20:36:23.077266Z",
          "iopub.status.idle": "2022-01-21T20:36:23.086174Z",
          "shell.execute_reply.started": "2022-01-21T20:36:23.077232Z",
          "shell.execute_reply": "2022-01-21T20:36:23.085313Z"
        },
        "trusted": true,
        "id": "PZdTMUtf_OBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:36:23.087505Z",
          "iopub.execute_input": "2022-01-21T20:36:23.087754Z",
          "iopub.status.idle": "2022-01-21T20:36:23.094726Z",
          "shell.execute_reply.started": "2022-01-21T20:36:23.08772Z",
          "shell.execute_reply": "2022-01-21T20:36:23.094023Z"
        },
        "trusted": true,
        "id": "I2GWiJbB_OBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classes(data_dir):\n",
        "    all_data = datasets.ImageFolder(data_dir)\n",
        "    return all_data.classes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:36:23.096074Z",
          "iopub.execute_input": "2022-01-21T20:36:23.096522Z",
          "iopub.status.idle": "2022-01-21T20:36:23.102743Z",
          "shell.execute_reply.started": "2022-01-21T20:36:23.096485Z",
          "shell.execute_reply": "2022-01-21T20:36:23.102053Z"
        },
        "trusted": true,
        "id": "JUzW-Jr4_OBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(data_dir, batch_size, train = False):\n",
        "    if train:\n",
        "        #train\n",
        "        transform = T.Compose([\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomVerticalFlip(),\n",
        "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD), # imagenet means\n",
        "            T.RandomErasing(p=0.1, value='random')\n",
        "        ])\n",
        "        train_data = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform = transform)\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        return train_loader, len(train_data)\n",
        "    else:\n",
        "        # val/test\n",
        "        transform = T.Compose([ # We dont need augmentation for test transforms\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD), # imagenet means\n",
        "        ])\n",
        "        val_data = datasets.ImageFolder(os.path.join(data_dir, \"valid/\"), transform=transform)\n",
        "        test_data = datasets.ImageFolder(os.path.join(data_dir, \"test/\"), transform=transform)\n",
        "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        return val_loader, test_loader, len(val_data), len(test_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:31.025131Z",
          "iopub.execute_input": "2022-01-21T20:44:31.025408Z",
          "iopub.status.idle": "2022-01-21T20:44:31.036312Z",
          "shell.execute_reply.started": "2022-01-21T20:44:31.025353Z",
          "shell.execute_reply": "2022-01-21T20:44:31.035507Z"
        },
        "trusted": true,
        "id": "lwEmzWJB_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/butterfly-images40-species/\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:32.174048Z",
          "iopub.execute_input": "2022-01-21T20:44:32.174543Z",
          "iopub.status.idle": "2022-01-21T20:44:32.179442Z",
          "shell.execute_reply.started": "2022-01-21T20:44:32.174502Z",
          "shell.execute_reply": "2022-01-21T20:44:32.178641Z"
        },
        "trusted": true,
        "id": "TEzLsqcW_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_loader, train_data_len) = get_data_loaders(dataset_path, 128, train=True)\n",
        "(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 32, train=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:32.278079Z",
          "iopub.execute_input": "2022-01-21T20:44:32.278304Z",
          "iopub.status.idle": "2022-01-21T20:44:32.510202Z",
          "shell.execute_reply.started": "2022-01-21T20:44:32.278279Z",
          "shell.execute_reply": "2022-01-21T20:44:32.509482Z"
        },
        "trusted": true,
        "id": "tdorODPN_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = get_classes(\"/kaggle/input/butterfly-images40-species/train/\")\n",
        "print(classes, len(classes))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:33.169553Z",
          "iopub.execute_input": "2022-01-21T20:44:33.170357Z",
          "iopub.status.idle": "2022-01-21T20:44:33.260095Z",
          "shell.execute_reply.started": "2022-01-21T20:44:33.170312Z",
          "shell.execute_reply": "2022-01-21T20:44:33.259354Z"
        },
        "trusted": true,
        "id": "p17M-rqo_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": val_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\": train_data_len,\n",
        "    \"val\": valid_data_len\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:33.296755Z",
          "iopub.execute_input": "2022-01-21T20:44:33.297046Z",
          "iopub.status.idle": "2022-01-21T20:44:33.300873Z",
          "shell.execute_reply.started": "2022-01-21T20:44:33.297018Z",
          "shell.execute_reply": "2022-01-21T20:44:33.30021Z"
        },
        "trusted": true,
        "id": "HPkEmKjG_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader), len(val_loader), len(test_loader))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:33.44599Z",
          "iopub.execute_input": "2022-01-21T20:44:33.446188Z",
          "iopub.status.idle": "2022-01-21T20:44:33.45212Z",
          "shell.execute_reply.started": "2022-01-21T20:44:33.446158Z",
          "shell.execute_reply": "2022-01-21T20:44:33.451267Z"
        },
        "trusted": true,
        "id": "qa2E8EvZ_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_len, valid_data_len, test_data_len)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:34.504967Z",
          "iopub.execute_input": "2022-01-21T20:44:34.50523Z",
          "iopub.status.idle": "2022-01-21T20:44:34.51047Z",
          "shell.execute_reply.started": "2022-01-21T20:44:34.505203Z",
          "shell.execute_reply": "2022-01-21T20:44:34.509771Z"
        },
        "trusted": true,
        "id": "dA3uDwiG_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, for the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:34.623106Z",
          "iopub.execute_input": "2022-01-21T20:44:34.623602Z",
          "iopub.status.idle": "2022-01-21T20:44:34.62943Z",
          "shell.execute_reply.started": "2022-01-21T20:44:34.623568Z",
          "shell.execute_reply": "2022-01-21T20:44:34.628542Z"
        },
        "trusted": true,
        "id": "qJkseml6_OBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\n",
        "MODEL_NAME = \"swin_tiny_patch4_window7_224\"\n",
        "# check hubconf for more models.\n",
        "model = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True) # load from torch hub"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:34.763108Z",
          "iopub.execute_input": "2022-01-21T20:44:34.764699Z",
          "iopub.status.idle": "2022-01-21T20:44:35.347609Z",
          "shell.execute_reply.started": "2022-01-21T20:44:34.764655Z",
          "shell.execute_reply": "2022-01-21T20:44:35.346841Z"
        },
        "trusted": true,
        "id": "1WBZ33kn_OBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters(): #freeze model\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_inputs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, len(classes))\n",
        ")\n",
        "model = model.to(device)\n",
        "print(model.head)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:35.77784Z",
          "iopub.execute_input": "2022-01-21T20:44:35.778403Z",
          "iopub.status.idle": "2022-01-21T20:44:35.833011Z",
          "shell.execute_reply.started": "2022-01-21T20:44:35.778344Z",
          "shell.execute_reply": "2022-01-21T20:44:35.832192Z"
        },
        "trusted": true,
        "id": "J-4MKBVg_OBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = LabelSmoothingCrossEntropy()\n",
        "criterion = criterion.to(device)\n",
        "optimizer = optim.AdamW(model.head.parameters(), lr=0.001)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:37.769978Z",
          "iopub.execute_input": "2022-01-21T20:44:37.770625Z",
          "iopub.status.idle": "2022-01-21T20:44:37.774838Z",
          "shell.execute_reply.started": "2022-01-21T20:44:37.770586Z",
          "shell.execute_reply": "2022-01-21T20:44:37.774058Z"
        },
        "trusted": true,
        "id": "uyuLN0Km_OBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr scheduler\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:39.726894Z",
          "iopub.execute_input": "2022-01-21T20:44:39.727319Z",
          "iopub.status.idle": "2022-01-21T20:44:39.731543Z",
          "shell.execute_reply.started": "2022-01-21T20:44:39.727287Z",
          "shell.execute_reply": "2022-01-21T20:44:39.730507Z"
        },
        "trusted": true,
        "id": "rSQA4tF6_OBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
        "            if phase == 'train':\n",
        "                model.train() # model to training mode\n",
        "            else:\n",
        "                model.eval() # model to evaluate\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step() # step at end of epoch\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
        "        print()\n",
        "    time_elapsed = time.time() - since # slight error\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:40.379017Z",
          "iopub.execute_input": "2022-01-21T20:44:40.379495Z",
          "iopub.status.idle": "2022-01-21T20:44:40.391583Z",
          "shell.execute_reply.started": "2022-01-21T20:44:40.379454Z",
          "shell.execute_reply": "2022-01-21T20:44:40.390784Z"
        },
        "trusted": true,
        "id": "c1C5sDCU_OBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=7) # now it is a lot faster\n",
        "# I will come back after 10 epochs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:44:41.514058Z",
          "iopub.execute_input": "2022-01-21T20:44:41.51452Z",
          "iopub.status.idle": "2022-01-21T20:49:08.10347Z",
          "shell.execute_reply.started": "2022-01-21T20:44:41.514465Z",
          "shell.execute_reply": "2022-01-21T20:49:08.10203Z"
        },
        "trusted": true,
        "id": "85TV5pqX_OBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "\n",
        "Ok, now we finished training. Lets run the dataset on the test loader and calculate accuracy"
      ],
      "metadata": {
        "id": "0-qWcgvk_OBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0 for i in range(len(classes)))\n",
        "class_total = list(0 for i in range(len(classes)))\n",
        "model_ft.eval()\n",
        "\n",
        "for data, target in tqdm(test_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    with torch.no_grad(): # turn off autograd for faster testing\n",
        "        output = model_ft(data)\n",
        "        loss = criterion(output, target)\n",
        "    test_loss = loss.item() * data.size(0)\n",
        "    _, pred = torch.max(output, 1)\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    if len(target) == 32:\n",
        "        for i in range(32):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "test_loss = test_loss / test_data_len\n",
        "print('Test Loss: {:.4f}'.format(test_loss))\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
        "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
        "        ))\n",
        "    else:\n",
        "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
        "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
        "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
        "        ))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:49:33.928216Z",
          "iopub.execute_input": "2022-01-21T20:49:33.929114Z",
          "iopub.status.idle": "2022-01-21T20:49:36.031589Z",
          "shell.execute_reply.started": "2022-01-21T20:49:33.929066Z",
          "shell.execute_reply": "2022-01-21T20:49:36.030814Z"
        },
        "trusted": true,
        "id": "AcKYhuB2_OBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our model earns 93% test accuracy, which is very high. lets save it\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "traced_script_module = torch.jit.trace(model.cpu(), example)\n",
        "traced_script_module.save(\"butterfly_swin_transformer.pt\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-21T20:41:32.10134Z",
          "iopub.execute_input": "2022-01-21T20:41:32.102178Z",
          "iopub.status.idle": "2022-01-21T20:41:35.145828Z",
          "shell.execute_reply.started": "2022-01-21T20:41:32.102135Z",
          "shell.execute_reply": "2022-01-21T20:41:35.145038Z"
        },
        "trusted": true,
        "id": "DZQjCCge_OBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# That's it for this video, see you next time"
      ],
      "metadata": {
        "id": "WB7tS1Dz_OBe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
