{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Using ü§ó Transformers for the first time | Pytorch",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center style=\"border-radius:10px;\n",
        "padding: 3rem 2rem;\n",
        "border: 3px solid #FCBE03;\n",
        "\">\n",
        "<h1 style=\"color:#FCBE03;\n",
        "font-size:3.0rem;\n",
        "margin:0;\n",
        "\">Using ü§ó Transformers for the first time</h1>\n",
        "<h2 style=\"color:#FCBE03;\n",
        "font-size:2.0rem;\n",
        "margin-top:1rem;\n",
        "margin-bottom:2.5rem;\n",
        "\">Finetuning bert-base-uncased</h2>\n",
        "<a href=\"https://kaggle.com/shreydan\" style=\"color: white;\n",
        "background-color: #FCBE03;\n",
        "border-radius: 25px;\n",
        "padding: 1rem 1.5rem;\n",
        "text-decoration: none;\n",
        "\">@shreydan</a>\n",
        "</center>"
      ],
      "metadata": {
        "id": "NTnrvTkHAZub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Hello everyone!** This is my first time using transformers and fine-tuning for a specific task. After many failed versions, referencing many notebooks and lot of time spent in docs later -- I was able to get a decent start with my transformers journey!"
      ],
      "metadata": {
        "id": "P0942ZPNAZue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I also made an EDA Notebook which you can check out here: [notebook](https://www.kaggle.com/code/shreydan/first-look-at-data-eda-read-some-essays)\n",
        "\n",
        "<center style=\"border-radius:10px;\n",
        "padding: 1rem;\n",
        "border: 3px solid #731eb0;\n",
        "\">\n",
        "<h3 style=\"color:#731eb0;\n",
        "\">üìëÔ∏è first look at data | EDA | read some essays</h3>\n",
        "</center>"
      ],
      "metadata": {
        "id": "1fTwSprqAZue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I finetuned more transformers after this, you can check out this public one:\n",
        "\n",
        "### [DeBERTa-v3-base | ü§óÔ∏è Accelerate | Finetuning Notebook](https://www.kaggle.com/code/shreydan/deberta-v3-base-accelerate-finetuning)\n",
        "\n",
        "- along with the Trainer class which you'll notice in this notebook, I integrated HuggingFace Accelerate -- which works as a helper to properly train transformers models. We do have to write our own custom training loops though!\n",
        "- I also learnt how to use transformers offline, since for inference submissions we need to turn off the internet access.\n",
        "- The reason I chose to use Accelerate was because my current training loops (the one in this notebook) are not very efficient, and even failed to execute with larger models such as deBERTa. Acclerate handles that with `prepare()` method to properly optimize the model, dataloaders, optimizer and scheduler.\n",
        "- Another optimization technique I used was `gradient accumulation`, check out my other notebook for implementation using Accelerate and other transformer training optimization resources.\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "RvVLbr_pAZuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #EAB209;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "- I used AutoModel: which let's me import a pre-trained model for inference/fine-tuning.\n",
        "- I used AutoTokenizer: a pre-trained tokenizer ofc but since its an AutoClass it  chooses the right tokenizer to use with the model. Pretty cool!!!\n",
        "- I chose to write the training loop from scratch using Pytorch\n",
        "- ~~Also if anyone can help me figure out the `tokenizers parallelism warning` and how to properly deal with it, please comment, I chose to set the environ flag for now.~~ I've noticed many notebooks just set this value to False.\n"
      ],
      "metadata": {
        "id": "bLZu7mWAAZuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "\n",
        "# ----------\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-10-13T15:55:13.421615Z",
          "iopub.execute_input": "2022-10-13T15:55:13.422017Z",
          "iopub.status.idle": "2022-10-13T15:55:14.791654Z",
          "shell.execute_reply.started": "2022-10-13T15:55:13.421936Z",
          "shell.execute_reply": "2022-10-13T15:55:14.790576Z"
        },
        "trusted": true,
        "id": "apTkUnIKAZuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print('transformers version:',transformers.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:08.725391Z",
          "iopub.execute_input": "2022-10-13T15:56:08.725917Z",
          "iopub.status.idle": "2022-10-13T15:56:08.732061Z",
          "shell.execute_reply.started": "2022-10-13T15:56:08.725884Z",
          "shell.execute_reply": "2022-10-13T15:56:08.730901Z"
        },
        "trusted": true,
        "id": "rUTv0sJ6AZug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #EDD608;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "## We'll train two times, with and without freezing the encoder\n",
        "\n",
        "- The model I used is the one in docs `bert-base-uncased`, uncased means the text has been converted to lower case during pre-tokenization\n",
        "- max_length is for the maximum sequence length and the model's max_length = 512 and most of the essay lengths were in that region.\n",
        "- The learning rate was crucial, I initally used lr=3e-4 but I wasn't getting any good results. After going through docs and searching for good learning rates for bert, order of 10**-5 seemed to be right and my results improved significantly\n",
        "- batch_size set to 16 coz anything more I had CUDA out of memory issues\n",
        "- scheduler: I used CosineAnnealingWarmRestarts because it was the one which was recommended in the docs\n",
        "- dropout: I learnt that dropout is bad for regression. Previously I had used 0.3 and 0.5, let's just turn it off to 0."
      ],
      "metadata": {
        "id": "CR47_4rjAZuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'model': 'bert-base-uncased',\n",
        "    'dropout': 0.,\n",
        "    'max_length': 512,\n",
        "    'batch_size': 16,\n",
        "    'epochs': 4,\n",
        "    'freeze_lr': 4e-4,\n",
        "    'unfreeze_lr': 2e-5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'scheduler': 'CosineAnnealingWarmRestarts'\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:12.826387Z",
          "iopub.execute_input": "2022-10-13T15:56:12.827582Z",
          "iopub.status.idle": "2022-10-13T15:56:12.892473Z",
          "shell.execute_reply.started": "2022-10-13T15:56:12.827538Z",
          "shell.execute_reply": "2022-10-13T15:56:12.891303Z"
        },
        "trusted": true,
        "id": "ZS8T_1FLAZui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer, Dataset and DataLoaders\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #E4F008;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "- After loading the tokenizer, I used the `encode_plus` method which converted the raw text to tokens as `input_ids` along with it we also get `token_type_ids` and `attention_mask`\n",
        "- `token_type_ids` are used in cases such in sequence classification and question-answering. [docs](https://huggingface.co/transformers/v3.2.0/glossary.html#token-type-ids)\n",
        "- `attention_mask` tells the model to what tokens to pay attention to and what to ignore.\n",
        "- we also truncate till first 512 tokens and pad to max_length\n",
        "- I used a simple pytorch dataset which takes the df, and generates samples as a dict.\n",
        "- the inputs dict: `{'input_ids','token_type_ids','attention_mask'}`\n",
        "- the targets dict: `{'labels'}` I treated this as a multiclass regression problem, the label is a vector of the 6 different types of scores in the dataset.\n",
        "- then simple dataloaders for the dataset"
      ],
      "metadata": {
        "id": "tjHjOrWEAZui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config['model'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:19.719928Z",
          "iopub.execute_input": "2022-10-13T15:56:19.720345Z",
          "iopub.status.idle": "2022-10-13T15:56:26.652315Z",
          "shell.execute_reply.started": "2022-10-13T15:56:19.720314Z",
          "shell.execute_reply": "2022-10-13T15:56:26.651344Z"
        },
        "trusted": true,
        "id": "5WYme8MSAZui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\n",
        "test_df = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:26.653999Z",
          "iopub.execute_input": "2022-10-13T15:56:26.65438Z",
          "iopub.status.idle": "2022-10-13T15:56:26.894677Z",
          "shell.execute_reply.started": "2022-10-13T15:56:26.654341Z",
          "shell.execute_reply": "2022-10-13T15:56:26.893458Z"
        },
        "trusted": true,
        "id": "qSieNu10AZuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:29.417394Z",
          "iopub.execute_input": "2022-10-13T15:56:29.418289Z",
          "iopub.status.idle": "2022-10-13T15:56:29.436722Z",
          "shell.execute_reply.started": "2022-10-13T15:56:29.418243Z",
          "shell.execute_reply": "2022-10-13T15:56:29.435719Z"
        },
        "trusted": true,
        "id": "1i4KnNzMAZuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.to_list())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:31.657751Z",
          "iopub.execute_input": "2022-10-13T15:56:31.658121Z",
          "iopub.status.idle": "2022-10-13T15:56:31.663626Z",
          "shell.execute_reply.started": "2022-10-13T15:56:31.658088Z",
          "shell.execute_reply": "2022-10-13T15:56:31.662582Z"
        },
        "trusted": true,
        "id": "bOkYXjNoAZuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EssayDataset:\n",
        "    def __init__(self, df, config, tokenizer=None, is_test=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.classes = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
        "        self.max_len = config['max_length']\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        sample = self.df['full_text'][idx]\n",
        "        tokenized = tokenizer.encode_plus(sample,\n",
        "                                          None,\n",
        "                                          add_special_tokens=True,\n",
        "                                          max_length=self.max_len,\n",
        "                                          truncation=True,\n",
        "                                          padding='max_length'\n",
        "                                         )\n",
        "        inputs = {\n",
        "            \"input_ids\": torch.tensor(tokenized['input_ids'], dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(tokenized['token_type_ids'], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(tokenized['attention_mask'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        if self.is_test == True:\n",
        "            return inputs\n",
        "\n",
        "        label = self.df.loc[idx,self.classes].to_list()\n",
        "        targets = {\n",
        "            \"labels\": torch.tensor(label, dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "        return inputs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:36.908173Z",
          "iopub.execute_input": "2022-10-13T15:56:36.908577Z",
          "iopub.status.idle": "2022-10-13T15:56:36.919828Z",
          "shell.execute_reply.started": "2022-10-13T15:56:36.908547Z",
          "shell.execute_reply": "2022-10-13T15:56:36.91858Z"
        },
        "trusted": true,
        "id": "w_qAYuZ2AZuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(df,test_size=0.2,random_state=1357,shuffle=True)\n",
        "print('dataframe shapes:',train_df.shape, val_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:38.698114Z",
          "iopub.execute_input": "2022-10-13T15:56:38.699043Z",
          "iopub.status.idle": "2022-10-13T15:56:38.709941Z",
          "shell.execute_reply.started": "2022-10-13T15:56:38.699007Z",
          "shell.execute_reply": "2022-10-13T15:56:38.708797Z"
        },
        "trusted": true,
        "id": "VuYeyBy1AZuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = EssayDataset(train_df, config, tokenizer=tokenizer)\n",
        "val_ds = EssayDataset(val_df, config, tokenizer=tokenizer)\n",
        "test_ds = EssayDataset(test_df, config, tokenizer=tokenizer, is_test=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:40.468023Z",
          "iopub.execute_input": "2022-10-13T15:56:40.46844Z",
          "iopub.status.idle": "2022-10-13T15:56:40.475791Z",
          "shell.execute_reply.started": "2022-10-13T15:56:40.468397Z",
          "shell.execute_reply": "2022-10-13T15:56:40.474635Z"
        },
        "trusted": true,
        "id": "8b-1U3RYAZul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0][0]['input_ids'].shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:42.716076Z",
          "iopub.execute_input": "2022-10-13T15:56:42.716557Z",
          "iopub.status.idle": "2022-10-13T15:56:42.737756Z",
          "shell.execute_reply.started": "2022-10-13T15:56:42.716517Z",
          "shell.execute_reply": "2022-10-13T15:56:42.736895Z"
        },
        "trusted": true,
        "id": "b_ywn6vJAZul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds,\n",
        "                                           batch_size=config['batch_size'],\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2,\n",
        "                                           pin_memory=True\n",
        "                                          )\n",
        "val_loader = torch.utils.data.DataLoader(val_ds,\n",
        "                                         batch_size=config['batch_size'],\n",
        "                                         shuffle=True,\n",
        "                                         num_workers=2,\n",
        "                                         pin_memory=True\n",
        "                                        )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:44.617486Z",
          "iopub.execute_input": "2022-10-13T15:56:44.617898Z",
          "iopub.status.idle": "2022-10-13T15:56:44.625347Z",
          "shell.execute_reply.started": "2022-10-13T15:56:44.617864Z",
          "shell.execute_reply": "2022-10-13T15:56:44.624032Z"
        },
        "trusted": true,
        "id": "OdWlqprnAZul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('loader shapes:',len(train_loader), len(val_loader))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:56:46.095318Z",
          "iopub.execute_input": "2022-10-13T15:56:46.095721Z",
          "iopub.status.idle": "2022-10-13T15:56:46.102537Z",
          "shell.execute_reply.started": "2022-10-13T15:56:46.095684Z",
          "shell.execute_reply": "2022-10-13T15:56:46.101261Z"
        },
        "trusted": true,
        "id": "jEAELTnfAZul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 : Unfreezed Encoder\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #C3F307;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "- the `encoder` returns 2 outputs, I turned off `return_dict` to unpack them. The first output is the last hidden state of the model, and the second output is pooled output of the model.\n",
        "- the outputs we get (BaseModelOutputWithPoolingAndCrossAttentions):\n",
        "    - last_hidden_state\n",
        "    - pooler_output\n",
        "    \n",
        "    ```python\n",
        "    >>> odict_keys(['last_hidden_state', 'pooler_output'])\n",
        "    >>> torch.Size([16, 768]) # 768 is our hidden_size\n",
        "    ```\n",
        "\n",
        "\n",
        "- we can also choose the freeze the `encoder` to train the fc layers but eh this is fine.\n",
        "- there are other pooling options we can use such as `mean pooling` which I'm yet to understand properly.\n",
        "- the pooled output size can be obtained through `model.config.hidden_size`\n",
        "- then simple fully-connected layers to get the logits we need\n"
      ],
      "metadata": {
        "id": "BBMLcBkyAZum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EssayModel(nn.Module):\n",
        "    def __init__(self,config,num_classes=6):\n",
        "        super(EssayModel,self).__init__()\n",
        "        self.model_name = config['model']\n",
        "        self.encoder = AutoModel.from_pretrained(self.model_name)\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.fc1 = nn.Linear(self.encoder.config.hidden_size,64)\n",
        "        self.fc2 = nn.Linear(64,num_classes)\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        _,outputs = self.encoder(**inputs, return_dict=False)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.fc1(outputs)\n",
        "        outputs = self.fc2(outputs)\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:57:53.421504Z",
          "iopub.execute_input": "2022-10-13T15:57:53.42187Z",
          "iopub.status.idle": "2022-10-13T15:57:53.428749Z",
          "shell.execute_reply.started": "2022-10-13T15:57:53.421841Z",
          "shell.execute_reply": "2022-10-13T15:57:53.427571Z"
        },
        "trusted": true,
        "id": "uFs9UQYSAZum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 : Freezed Encoder\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #C3F307;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "- Freezing the model also optimizes the training, takes less GPU memory, and reduces overfitting, It's really easy to overfit transformers. I have 30+ versions to vouch for that, freezing the base model really helps"
      ],
      "metadata": {
        "id": "JSoF4ozEAZum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrozenEssayModel(nn.Module):\n",
        "    def __init__(self,config,num_classes=6):\n",
        "        super(FrozenEssayModel,self).__init__()\n",
        "        self.model_name = config['model']\n",
        "        self.encoder = AutoModel.from_pretrained(self.model_name)\n",
        "\n",
        "        # this is how you freeze a model: the base_model is generic term for the transformer name\n",
        "        for param in self.encoder.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.fc1 = nn.Linear(self.encoder.config.hidden_size,64)\n",
        "        self.fc2 = nn.Linear(64,num_classes)\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        _,outputs = self.encoder(**inputs, return_dict=False)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.fc1(outputs)\n",
        "        outputs = self.fc2(outputs)\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T15:59:14.678142Z",
          "iopub.execute_input": "2022-10-13T15:59:14.678924Z",
          "iopub.status.idle": "2022-10-13T15:59:14.687532Z",
          "shell.execute_reply.started": "2022-10-13T15:59:14.678886Z",
          "shell.execute_reply": "2022-10-13T15:59:14.686521Z"
        },
        "trusted": true,
        "id": "5XuWs3sNAZum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #A2F606;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "- **Loss Function:** [MCRMSE](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/348985)\n",
        "- **Optimizer:** AdamW, default weight-decay: 1e-2 -- huggingface docs recommend modifying the weight decay for params hence the `get_optim` method\n",
        "- **Scheduler:** -- you can change the scheduler in the config and Trainer class will perform the scheduler.step() where it's supposed to.\n",
        "    - ReduceLROnPlateau:: min_lr: 1e-7\n",
        "    - **CosineAnnealingWithWarmRestarts::** T_0: 2, eta_min: 1e-7\n",
        "    - StepLR:: step_size: 2\n",
        "- **steps in every epoch:**\n",
        "    - set model to train mode\n",
        "    - train one epoch\n",
        "        - get inputs, targets\n",
        "        - move them to GPU\n",
        "        - calculate loss\n",
        "        - backprop\n",
        "        - optim step\n",
        "        - scheduler step\n",
        "    - garbage collection and clear cuda cache\n",
        "    - set model to eval mode\n",
        "    - validate one epoch\n",
        "        - get inputs, targets\n",
        "        - move them to GPU\n",
        "        - calculate loss\n",
        "        - scheduler step if scheduler is ReduceLROnPlateau\n",
        "    - garbage collection and clear cuda cache"
      ],
      "metadata": {
        "id": "iji54F1SAZum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, loaders, config,lr='unfreeze'):\n",
        "        self.model = model\n",
        "        self.train_loader, self.val_loader = loaders\n",
        "        self.config = config\n",
        "        self.input_keys = ['input_ids','token_type_ids','attention_mask']\n",
        "\n",
        "        self.loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "        if lr == 'unfreeze':\n",
        "            self.lr = self.config['unfreeze_lr']\n",
        "        else:\n",
        "            self.lr = self.config['freeze_lr']\n",
        "\n",
        "        self.optim = self._get_optim()\n",
        "\n",
        "\n",
        "        self.scheduler_options = {\n",
        "            'CosineAnnealingWarmRestarts': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optim, T_0=5,eta_min=1e-7),\n",
        "            'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau(self.optim, 'min', min_lr=1e-7),\n",
        "            'StepLR': torch.optim.lr_scheduler.StepLR(self.optim,step_size=2)\n",
        "        }\n",
        "\n",
        "        self.scheduler = self.scheduler_options[self.config['scheduler']]\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_mcrmse = []\n",
        "\n",
        "    def _get_optim(self):\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "    def mcrmse(self, outputs, targets):\n",
        "        colwise_mse = torch.mean(torch.square(targets - outputs), dim=0)\n",
        "        loss = torch.mean(torch.sqrt(colwise_mse), dim=0)\n",
        "        return loss\n",
        "\n",
        "    def train_one_epoch(self,epoch):\n",
        "\n",
        "        running_loss = 0.\n",
        "        progress = tqdm(self.train_loader, total=len(self.train_loader))\n",
        "\n",
        "        for i,(inputs,targets) in enumerate(progress):\n",
        "\n",
        "            self.optim.zero_grad()\n",
        "\n",
        "            inputs = {k:inputs[k].to(device=config['device']) for k in inputs.keys()}\n",
        "            targets = targets['labels'].to(device=config['device'])\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "\n",
        "            loss = self.loss_fn(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            self.optim.step()\n",
        "\n",
        "            if self.config['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
        "                self.scheduler.step(epoch-1+i/len(self.train_loader)) # as per pytorch docs\n",
        "\n",
        "            del inputs, targets, outputs, loss\n",
        "\n",
        "        if self.config['scheduler'] == 'StepLR':\n",
        "            self.scheduler.step()\n",
        "\n",
        "        train_loss = running_loss/len(self.train_loader)\n",
        "        self.train_losses.append(train_loss)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def valid_one_epoch(self,epoch):\n",
        "\n",
        "        running_loss = 0.\n",
        "        running_mcrmse = 0.\n",
        "        progress = tqdm(self.val_loader, total=len(self.val_loader))\n",
        "\n",
        "        for (inputs, targets) in progress:\n",
        "\n",
        "            inputs = {k:inputs[k].to(device=config['device']) for k in inputs.keys()}\n",
        "            targets = targets['labels'].to(device=config['device'])\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "\n",
        "            loss = self.loss_fn(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            running_mcrmse += self.mcrmse(outputs, targets).item()\n",
        "\n",
        "            del inputs, targets, outputs, loss\n",
        "\n",
        "\n",
        "        val_loss = running_loss/len(self.val_loader)\n",
        "        self.val_losses.append(val_loss)\n",
        "\n",
        "        self.val_mcrmse.append(running_mcrmse/len(self.val_loader))\n",
        "        del running_mcrmse\n",
        "\n",
        "        if config['scheduler'] == 'ReduceLROnPlateau':\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "    def test(self, test_loader):\n",
        "\n",
        "        preds = []\n",
        "        for (inputs) in test_loader:\n",
        "            inputs = {k:inputs[k].to(device=config['device']) for k in inputs.keys()}\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "            preds.append(outputs.detach().cpu())\n",
        "\n",
        "        preds = torch.concat(preds)\n",
        "        return preds\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        fit_progress = tqdm(\n",
        "            range(1, self.config['epochs']+1),\n",
        "            leave = True,\n",
        "            desc=\"Training...\"\n",
        "        )\n",
        "\n",
        "        for epoch in fit_progress:\n",
        "\n",
        "            self.model.train()\n",
        "            fit_progress.set_description(f\"EPOCH {epoch} / {self.config['epochs']} | training...\")\n",
        "            self.train_one_epoch(epoch)\n",
        "            self.clear()\n",
        "\n",
        "            self.model.eval()\n",
        "            fit_progress.set_description(f\"EPOCH {epoch} / {self.config['epochs']} | validating...\")\n",
        "            self.valid_one_epoch(epoch)\n",
        "            self.clear()\n",
        "\n",
        "            print(f\"{'-'*30} EPOCH {epoch} / {self.config['epochs']} {'-'*30}\")\n",
        "            print(f\"train loss: {self.train_losses[-1]}\")\n",
        "            print(f\"valid loss: {self.val_losses[-1]}\\n\\n\")\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T16:03:57.945825Z",
          "iopub.execute_input": "2022-10-13T16:03:57.94618Z",
          "iopub.status.idle": "2022-10-13T16:03:57.970256Z",
          "shell.execute_reply.started": "2022-10-13T16:03:57.946149Z",
          "shell.execute_reply": "2022-10-13T16:03:57.969231Z"
        },
        "trusted": true,
        "id": "IgHsbUnuAZun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #7FF906;margin-bottom: 1rem;\"></div>"
      ],
      "metadata": {
        "id": "hY5lHFcwAZun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unfreezed Encoder Model"
      ],
      "metadata": {
        "id": "r1eldZAZAZun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = EssayModel(config).to(device=config['device'])\n",
        "trainer_unfreeze = Trainer(model, (train_loader, val_loader), config, lr='unfreeze')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T16:04:02.914435Z",
          "iopub.execute_input": "2022-10-13T16:04:02.914788Z",
          "iopub.status.idle": "2022-10-13T16:04:05.368127Z",
          "shell.execute_reply.started": "2022-10-13T16:04:02.914757Z",
          "shell.execute_reply": "2022-10-13T16:04:05.367158Z"
        },
        "trusted": true,
        "id": "M6b9n8qnAZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_unfreeze.fit()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T16:04:14.07577Z",
          "iopub.execute_input": "2022-10-13T16:04:14.076126Z",
          "iopub.status.idle": "2022-10-13T16:10:04.262256Z",
          "shell.execute_reply.started": "2022-10-13T16:04:14.076088Z",
          "shell.execute_reply": "2022-10-13T16:10:04.261061Z"
        },
        "trusted": true,
        "id": "15Yqo-XdAZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "2GiO-Ip5AZuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freeing up GPU memory\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T16:11:14.672685Z",
          "iopub.execute_input": "2022-10-13T16:11:14.673081Z",
          "iopub.status.idle": "2022-10-13T16:11:14.678189Z",
          "shell.execute_reply.started": "2022-10-13T16:11:14.673047Z",
          "shell.execute_reply": "2022-10-13T16:11:14.676984Z"
        },
        "trusted": true,
        "id": "3W0ZNL0dAZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "ziqKKn0YAZuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freezed Encoder Model"
      ],
      "metadata": {
        "id": "UgwrzQLjAZuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FrozenEssayModel(config).to(device=config['device'])\n",
        "trainer_freeze = Trainer(model, (train_loader, val_loader), config, lr='freeze')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T16:12:39.272155Z",
          "iopub.execute_input": "2022-10-13T16:12:39.272676Z",
          "iopub.status.idle": "2022-10-13T16:12:42.16917Z",
          "shell.execute_reply.started": "2022-10-13T16:12:39.272636Z",
          "shell.execute_reply": "2022-10-13T16:12:42.168084Z"
        },
        "trusted": true,
        "id": "hsj2jq1WAZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_freeze.fit()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-13T16:12:45.769196Z",
          "iopub.execute_input": "2022-10-13T16:12:45.769582Z",
          "iopub.status.idle": "2022-10-13T16:15:05.243903Z",
          "shell.execute_reply.started": "2022-10-13T16:12:45.769551Z",
          "shell.execute_reply": "2022-10-13T16:15:05.24278Z"
        },
        "trusted": true,
        "id": "_AtO-KjqAZup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #EAB209;margin-bottom: 1rem;\"></div>"
      ],
      "metadata": {
        "id": "xVnO3yDGAZup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unfreezed Encoder Model"
      ],
      "metadata": {
        "id": "8SOIIC8GAZup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses_df = pd.DataFrame({'epoch':list(range(1,config['epochs'] + 1)),\n",
        "                          'train_loss':trainer_unfreeze.train_losses,\n",
        "                          'val_loss': trainer_unfreeze.val_losses,\n",
        "                          'val_mcrmse': trainer_unfreeze.val_mcrmse\n",
        "                         })\n",
        "losses_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-01T19:36:38.731831Z",
          "iopub.execute_input": "2022-10-01T19:36:38.732547Z",
          "iopub.status.idle": "2022-10-01T19:36:38.741248Z",
          "shell.execute_reply.started": "2022-10-01T19:36:38.73251Z",
          "shell.execute_reply": "2022-10-01T19:36:38.740214Z"
        },
        "trusted": true,
        "id": "zQ7qW3bcAZup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freezed Encoder Model"
      ],
      "metadata": {
        "id": "bodFFt5RAZuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses_df = pd.DataFrame({'epoch':list(range(1,config['epochs'] + 1)),\n",
        "                          'train_loss':trainer_freeze.train_losses,\n",
        "                          'val_loss': trainer_freeze.val_losses,\n",
        "                          'val_mcrmse': trainer_freeze.val_mcrmse\n",
        "                         })\n",
        "losses_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-01T19:36:38.756032Z",
          "iopub.execute_input": "2022-10-01T19:36:38.756392Z",
          "iopub.status.idle": "2022-10-01T19:36:38.77209Z",
          "shell.execute_reply.started": "2022-10-01T19:36:38.756354Z",
          "shell.execute_reply": "2022-10-01T19:36:38.771034Z"
        },
        "trusted": true,
        "id": "6pZmlnxcAZuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #EDD608;margin-bottom: 1rem;\"></div>"
      ],
      "metadata": {
        "id": "4N91fmouAZuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(trainer_unfreeze.train_losses, color='red')\n",
        "plt.plot(trainer_unfreeze.val_losses, color='orange')\n",
        "plt.title('MCRMSE Loss: Unfreezed Encoder')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-01T19:36:38.773409Z",
          "iopub.execute_input": "2022-10-01T19:36:38.774056Z",
          "iopub.status.idle": "2022-10-01T19:36:38.99092Z",
          "shell.execute_reply.started": "2022-10-01T19:36:38.77402Z",
          "shell.execute_reply": "2022-10-01T19:36:38.990014Z"
        },
        "trusted": true,
        "id": "TEGMN3ibAZuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(trainer_freeze.train_losses, color='red')\n",
        "plt.plot(trainer_freeze.val_losses, color='orange')\n",
        "plt.title('MCRMSE Loss: Freezed Encoder')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4mkAyiZVAZuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #0DFC25;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "As you can see, I also tried the classic NLP methods, they were from scratch as well, even the embedding layer, could've used stuff like Word2Vec but I'm new to NLP so went with scratch implementations and ofcourse the results are not that good. Using a Byte-Pair encoding tokenizer improved the results slightly which was good to see, I used huggingface tokenizer for that as well but not pre-trained ofc.\n",
        "\n",
        "## The notebooks:\n",
        "\n",
        "- [LSTM + Embeddings](https://www.kaggle.com/code/shreydan/lstm-embeddings)\n",
        "- [BPE Tokenizer + LSTM + Embeddings](https://www.kaggle.com/code/shreydan/bpe-tokenizer-lstm-embeddings)\n",
        "\n",
        "![lstm](https://i.imgur.com/6vAeO6a.jpeg)\n"
      ],
      "metadata": {
        "id": "eUk1Upr7AZuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing\n",
        "<div style=\"width:100%;height:0;border-bottom: 3px solid #EAB209;margin-bottom: 1rem;\"></div>\n",
        "\n",
        "## Thank you for taking the time to go through my notebook!\n",
        "\n",
        "- Would appreciate feedback in the comments, please do let me know what I could've done better, how I can improve my training loops and especially using regarding usage of transformers.\n",
        "- **Upvote if you liked this notebook, really helps a lot as I'm currently new to NLP.**\n",
        "- **Follow if you like my work! :)**\n",
        "\n",
        "## References:\n",
        "\n",
        "- [FB3: RoBERTa PyTorch Baseline - KFolds + W&B üöÄ by @heyytanay](https://www.kaggle.com/code/heyytanay/fb3-roberta-pytorch-baseline-kfolds-w-b) - his work is amazing, I learnt a lot from just reading his notebooks since I joined kaggle! Do checkout his work.\n",
        "- [HuggingFace AutoClass](https://huggingface.co/docs/transformers/main/autoclass_tutorial)\n",
        "- [HuggingFace training & Finetuning with Pytorch](https://huggingface.co/transformers/v3.3.1/training.html#pytorch)\n",
        "- [tez: fb3 training kernel by @abhishek](https://www.kaggle.com/code/abhishek/tez-fb3-training-kernel) - he basically taught me Pytorch, creating datasets andwriting training loops. Checkout his youtube tutorials, pretty cool!\n",
        "- [HuggingFace training](https://huggingface.co/docs/transformers/main/training)\n",
        "\n",
        "___\n",
        "\n",
        "Matthew 5:9"
      ],
      "metadata": {
        "id": "myfgNJ8VAZuv"
      }
    }
  ]
}